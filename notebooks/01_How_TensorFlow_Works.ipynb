{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How TensorFlow Works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "\n",
    "The complexity of the financial markets has forced to create trading strategies based on artificial intelligence (AI) models. The last ones requires a large amount of computing and deep learning algorithms can easily need tens of millions of parameters and billions of connections. The algorithmic trading is full of data and calculations with the data. To deal with it, tensors (multidimensional data arrays) are ideal mathematical entities. And Tensorflow is the right software to use tensors. The training and use of those models require enormous computational resources, in addition, the TensorFlow library allows one to concentrate on the creativity of its solution and leave the infrastructure aside.\n",
    "\n",
    "TensorFlow was open-sourced in November 2015.  Since the inception date, TensorFlow has become Github's most prominent machine learning repository. (https://github.com/tensorflow/tensorflow)\n",
    "\n",
    "TensorFlow's popularity is due to many things, but mainly because of the computational graph concept and the adaptability of the Tensorflow python API structure.  This makes solving real problems with TensorFlow accessible to most programmers, even the beginner ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How TensorFlow Operates\n",
    "\n",
    "Basics of tensorflow is that first we create a model which is called a computational graph with tensorflow objects then we create a tensorflow session in which we start running all the computation. This tutorial will talk you through pseudocode of how a Tensorflow algorithm usually works.\n",
    "\n",
    "Tensorflow is supported on the three principal OS systems (Windows, Linux, and Mac). Throughout these jupyter notebooks we will only concern ourselves with the Python library wrapper of Tensorflow. This book will use Python 3.X (https://www.python.org) and Tensorflow 0.10+ (https://www.tensorflow.org).  Tensorflow can run on the CPU, but it runs faster if it runs on the GPU, and it is supported on graphics cards with NVidia Compute Capability 3.0+. To run on a GPU, you will also need to download and install the NVidia Cuda Toolkit (https://developer.nvidia.com/cuda-downloads).\n",
    "\n",
    "As usual, we use Conda environments to develope our code (https://github.com/parrondo/quant-trading-project-structure). Please see the `environment.yml` in the main directory of this repository and run the command \n",
    "`conda env create --file environment.yml` to guarentee that all the necessary libraries are available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General TensorFlow Algorithm Workflow\n",
    "\n",
    "Here we introduce the general workflow of Tensorflow Algorithms. This workflow can be follow as a template.\n",
    "\n",
    "1. Load configuration.\n",
    "\n",
    "    This is usually the first step. Here you import libraries and modules as needed. Also load environment variables and configuration files.\n",
    "\n",
    "\n",
    "2. Ingest data.\n",
    "\n",
    "    All of machine learning algorithms depend on data. So,  we either generate data or use an outside source of data. Sometimes it is better to rely on generated data because we will want to test the expected outcome. Most times we will access market data sets for the given research. in any case, it is convenient to have a well defined ingestion data model as we provide in this tutorial.\n",
    "    \n",
    "    Output: raw dataset files under \"data/raw\" folder.   \n",
    "\n",
    "\n",
    "3. Basic pre-process data.\n",
    "\n",
    "    The raw dataset usually has faults which dificult the next steps. In this steps we proceed to clean data, manage missing data, define features and labels, encode the dependent variable and dataset time alignment when necessary.\n",
    "\n",
    "\n",
    "4. Split data\n",
    "\n",
    "    This step is useful when you need to separate data into training and test sets. We can also customize the way the data is divided. Sometimes we need to support data randomization; others split methods are designed for a certain type of data or model type.\n",
    "    \n",
    "    Output: two dataset training dataset and test dataset, usually they are resident in memory but in case we need to save them, then \"data/interim\" is our folder.   \n",
    "\n",
    "\n",
    "5. Transform features.\n",
    "\n",
    "    In general, the data is not in the correct dimension, structure or type expected by our Tensorflow trading algorithms. We have to transform the raw or provisional (interim) data before we can use them. Most algorithms also expect standardized (normalized) data and we will do this here as well. Tensorflow has built-in functions that can normalize the data for you.\n",
    "\n",
    "    `data = tf.nn.batch_norm_with_global_normalization(...)`\n",
    "    \n",
    "    Caution! Some algorithms require that the data be normalized before training a model. Other algorithms, on the other hand, perform their own data scale or normalization. So, when choosing an automatic learning algorithm to use in a predictive model, be sure to review the algorithm data requirements before applying the normalization to the training data.\n",
    "    \n",
    "    Dimension reduction, when is needed, is also included at this stage.\n",
    "    \n",
    "    Finally, in this step, we must have clear what will be the structure (dimensions) of the tensors that are involved in the input of data and in all calculations.\n",
    "    \n",
    "    Output: two dataset transformed training dataset and transformed test dataset. My be this step is acomplished several times given several pairs of train-test datasets (i.e. normalized dataset, PCA dataset, standardised dataset,...)\n",
    "\n",
    "\n",
    "6. Implement the model\n",
    "\n",
    "    Several sub-process expected here, describing as follow:\n",
    "   \n",
    "    *Set algorithm parameters.*\n",
    "\n",
    "    Algorithms usually have a set of parameters that we hold constant throughout the procedure (i.e. the number of iterations, the learning rate, or other fixed parameters).  It is considered good practice to initialize these together so the user can easily find them.\n",
    "\n",
    "    ```python\n",
    "    learning_rate = 0.005\n",
    "    a = b\n",
    "    iterations = 1000\n",
    "    epochs=50\n",
    "    ```\n",
    "\n",
    "    *Initialize variables and placeholders.*\n",
    "\n",
    "    we have to tell Tensorflow what it can and cannot modify.  Tensorflow will modify the variables during optimization to minimize a loss function.  To accomplish this, we feed in data through placeholders.  Placeholder simply allocates block of memory for future use. By default, placeholder has an unconstrained shape, which allows us to feed tensors of different shapes in a session. We need to initialize variables and define size and type of placeholders, so that Tensorflow knows what to expect.\n",
    "\n",
    "    ```python\n",
    "    k_var = tf.constant(50)\n",
    "    x_train = tf.placeholder(tf.float32, [None, input_size])\n",
    "    y_train = tf.placeholder(tf.fload32, [None, num_classes])\n",
    "    ```\n",
    "\n",
    "    *Define the model structure.*\n",
    "\n",
    "    After we have the data, and initialized variables and set placeholders, we have to define the model.  This is done by bmean of the powerfull concept of a computational graph.  The graph nodes represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) that flow between them. We tell Tensorflow what operations must be done on the variables and placeholders to get our model predictions.\n",
    "    \n",
    "    Most TensorFlow programs start with a dataflow graph construction phase. In this phase, we invoke TensorFlow API functions that construct new tf.Operation (node) and tf.Tensor (edge) objects and add them to a tf.Graph instance. \n",
    "\n",
    "    ```python\n",
    "    y_pred = tf.add(tf.mul(x_input, weight_matrix), b_matrix)\n",
    "    ```\n",
    "    \n",
    "    *Set loss functions.*\n",
    "\n",
    "    After defining the model, we must be able to evaluate the output.  THere we set the loss function.  The loss function is very important as it tells us how far off our predictions are from the actual values. There are several types of loss functions .\n",
    "\n",
    "    ```python\n",
    "    loss = tf.reduce_mean(tf.square(y_actual â€“ y_pred))\n",
    "    ```\n",
    "\n",
    "\n",
    "7. Train the model.\n",
    "\n",
    "    Now that we have everything in place, we create an instance or our computational graph and feed in the data through the placeholders and let Tensorflow change the variables to predict our training data. TensorFlow provides a default graph that is an implicit argument to all API functions in the same context. Here is one way to initialize the computational graph.\n",
    "\n",
    "    ```python\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        ...\n",
    "\t    session.run(...)\n",
    "        ...\n",
    "    ```\n",
    "\n",
    "    Note that we can also initiate our graph with\n",
    "\n",
    "    ```python\n",
    "    # Using the \"close()\" method.\n",
    "    sess = tf.Session(graph=graph)\n",
    "    sess.run(...)\n",
    "    sess.close()        \n",
    "    ...\n",
    "    ```\n",
    "    Output: Trained model which is stored in the folder \"models\"   \n",
    "\n",
    "\n",
    "8. Evaluate the model.\n",
    "\n",
    "    Once we have built and trained the model, we should evaluate the model by looking at how well it does on new data  known as test data.\n",
    "\n",
    "\n",
    "9. Hyperparameter optimization\n",
    "\n",
    "    This is not a mandatory step but it is convenient. The initial neural network is probably not the optimal one. So here we can tweak a bit in the parameters of the network to try to improve them. Then train an evaluate again and again until meet the optimization condition. As result we get the final selected network.\n",
    "    \n",
    "    Output: Final selected trained model which is stored in the folder \"models\"\n",
    "    \n",
    "\n",
    "10. Predict.\n",
    "\n",
    "    Yeees, this is the climax of our work!. We want to predict as much as possible, It is also important to know how to make predictions on new, unseen, data.  We can do this with all the models, once we have them trained. We could say that this is the goal of all our algorithmic trading efforts.\n",
    "    \n",
    "    Output: A prediction. This will help us what to do with a selected financial instrument: Buy, Hold, Sell,..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "TensorFlow is an open source software library for numerical computation using data flow graphs. To work with it, we have to setup the data, variables, placeholders, and model before we tell the program to train. Tensorflow accomplishes this through the computational graph. The graph nodes represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) that flow between them. We tell it to minimize a loss function and Tensorflow does this by modifying the variables in the model.  Tensorflow knows how to modify the variables because it keeps track of the computations in the model and automatically calculates the gradients for every variable.\n",
    "\n",
    "TensorFlow algorithms are designed to have a cyclic workflow.  We set up this cycle as a computational graph and (1) feed in data through the placeholders, (2) calculate the output of the computational graph, (3) compare the output to the desired output with the aidd of a loss function, (4) modify the model variables according to the automatic back propagation, and finally (5) repeat the process until a stopping criteria is met. (6) Then we evaluate the trained model and if we are confortable with it finally (6) we make predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Supervised learning](../images/supervised_learning_flowchart_avatar.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeptrading",
   "language": "python",
   "name": "deeptrading"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
