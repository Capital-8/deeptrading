{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a multiple hidden layer Neural Network \n",
    "\n",
    "\n",
    "![deep_network_model](../images/deep_network_model_avatar.jpg)\n",
    "\n",
    "\n",
    "The progress of the model can be saved during and after training. This means that a model can be resumed where it left off and avoid long training times. Saving also means that you can share your model and others can recreate your work.\n",
    "\n",
    "We will illustrate how to create a multiple fully connected hidden layer NN, save it and make predictions with trained model after reload it.\n",
    "\n",
    "We will use the iris data for this exercise.\n",
    "\n",
    "We will build a three-hidden layer neural network  to predict the fourth attribute, Petal Width from the other three (Sepal length, Sepal width, Petal length)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parrondo/anaconda3/envs/deeptrading/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/parrondo/anaconda3/envs/deeptrading/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/parrondo/anaconda3/envs/deeptrading/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/parrondo/anaconda3/envs/deeptrading/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_iris\n",
    "from tensorflow.python.framework import ops\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of dataset\n",
      "n= 150 p= 3\n"
     ]
    }
   ],
   "source": [
    "# Before getting into pandas dataframes we will load an example dataset from sklearn library \n",
    "# type(data) #iris is a bunch instance which is inherited from dictionary\n",
    "data = load_iris() #load iris dataset\n",
    "\n",
    "# We get a pandas dataframe to better visualize the datasets\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "\n",
    "X_raw = np.array([x[0:3] for x in data.data])\n",
    "y_raw = np.array([x[3] for x in data.data])\n",
    "\n",
    "# Dimensions of dataset\n",
    "print(\"Dimensions of dataset\")\n",
    "n = X_raw.shape[0]\n",
    "p = X_raw.shape[1]\n",
    "print(\"n=\",n,\"p=\",p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['target_names', 'target', 'DESCR', 'data', 'feature_names'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys() #keys of the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_raw.shape # Array 150x3. Each element is a 3-dimensional data point: sepal length, sepal width, petal length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_raw.shape # Vector 150. Each element is a 1-dimensional (scalar) data point: petal width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.7</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>5.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.9</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>6.2</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>6.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.8</td>\n",
       "      <td>1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>7.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.1</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>7.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>6.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>6.4</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>6.8</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.9</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                  5.1               3.5                1.4               0.2\n",
       "1                  4.9               3.0                1.4               0.2\n",
       "2                  4.7               3.2                1.3               0.2\n",
       "3                  4.6               3.1                1.5               0.2\n",
       "4                  5.0               3.6                1.4               0.2\n",
       "5                  5.4               3.9                1.7               0.4\n",
       "6                  4.6               3.4                1.4               0.3\n",
       "7                  5.0               3.4                1.5               0.2\n",
       "8                  4.4               2.9                1.4               0.2\n",
       "9                  4.9               3.1                1.5               0.1\n",
       "10                 5.4               3.7                1.5               0.2\n",
       "11                 4.8               3.4                1.6               0.2\n",
       "12                 4.8               3.0                1.4               0.1\n",
       "13                 4.3               3.0                1.1               0.1\n",
       "14                 5.8               4.0                1.2               0.2\n",
       "15                 5.7               4.4                1.5               0.4\n",
       "16                 5.4               3.9                1.3               0.4\n",
       "17                 5.1               3.5                1.4               0.3\n",
       "18                 5.7               3.8                1.7               0.3\n",
       "19                 5.1               3.8                1.5               0.3\n",
       "20                 5.4               3.4                1.7               0.2\n",
       "21                 5.1               3.7                1.5               0.4\n",
       "22                 4.6               3.6                1.0               0.2\n",
       "23                 5.1               3.3                1.7               0.5\n",
       "24                 4.8               3.4                1.9               0.2\n",
       "25                 5.0               3.0                1.6               0.2\n",
       "26                 5.0               3.4                1.6               0.4\n",
       "27                 5.2               3.5                1.5               0.2\n",
       "28                 5.2               3.4                1.4               0.2\n",
       "29                 4.7               3.2                1.6               0.2\n",
       "..                 ...               ...                ...               ...\n",
       "120                6.9               3.2                5.7               2.3\n",
       "121                5.6               2.8                4.9               2.0\n",
       "122                7.7               2.8                6.7               2.0\n",
       "123                6.3               2.7                4.9               1.8\n",
       "124                6.7               3.3                5.7               2.1\n",
       "125                7.2               3.2                6.0               1.8\n",
       "126                6.2               2.8                4.8               1.8\n",
       "127                6.1               3.0                4.9               1.8\n",
       "128                6.4               2.8                5.6               2.1\n",
       "129                7.2               3.0                5.8               1.6\n",
       "130                7.4               2.8                6.1               1.9\n",
       "131                7.9               3.8                6.4               2.0\n",
       "132                6.4               2.8                5.6               2.2\n",
       "133                6.3               2.8                5.1               1.5\n",
       "134                6.1               2.6                5.6               1.4\n",
       "135                7.7               3.0                6.1               2.3\n",
       "136                6.3               3.4                5.6               2.4\n",
       "137                6.4               3.1                5.5               1.8\n",
       "138                6.0               3.0                4.8               1.8\n",
       "139                6.9               3.1                5.4               2.1\n",
       "140                6.7               3.1                5.6               2.4\n",
       "141                6.9               3.1                5.1               2.3\n",
       "142                5.8               2.7                5.1               1.9\n",
       "143                6.8               3.2                5.9               2.3\n",
       "144                6.7               3.3                5.7               2.5\n",
       "145                6.7               3.0                5.2               2.3\n",
       "146                6.3               2.5                5.0               1.9\n",
       "147                6.5               3.0                5.2               2.0\n",
       "148                6.2               3.4                5.4               2.3\n",
       "149                5.9               3.0                5.1               1.8\n",
       "\n",
       "[150 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Leave in blanck intentionally\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples:  150 \n",
      "Samples in train set:  105 \n",
      "Samples in test set:  45\n",
      "X_train.shape =  (105, 3) y_train.shape = (105,) \n",
      "X_test.shape =   (45, 3) y_test.shape =  (45,)\n"
     ]
    }
   ],
   "source": [
    "# split into train and test sets\n",
    "\n",
    "# Total samples\n",
    "nsamples = n\n",
    "\n",
    "# Splitting into train (70%) and test (30%) sets\n",
    "split = 70 # training split% ; test (100-split)%\n",
    "jindex = nsamples*split//100 # Index for slicing the samples\n",
    "\n",
    "# Samples in train\n",
    "nsamples_train = jindex\n",
    "\n",
    "# Samples in test\n",
    "nsamples_test = nsamples - nsamples_train\n",
    "print(\"Total number of samples: \",nsamples,\"\\nSamples in train set: \", nsamples_train,\n",
    "      \"\\nSamples in test set: \",nsamples_test)\n",
    "\n",
    "# Here are train and test samples\n",
    "X_train = X_raw[:jindex, :]\n",
    "y_train = y_raw[:jindex]\n",
    "\n",
    "X_test = X_raw[jindex:, :]\n",
    "y_test = y_raw[jindex:]\n",
    "\n",
    "print(\"X_train.shape = \", X_train.shape, \"y_train.shape =\", y_train.shape, \"\\nX_test.shape =  \",\n",
    "      X_test.shape, \"y_test.shape = \", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**\n",
    "\n",
    "Becareful not to write `X_test_std = sc.fit_transform(X_test)` instead of `X_test_std = sc.transform(X_test)`. In this case, it wouldn't make a great difference since the mean and standard deviation of the test set should be (quite) similar to the training set. However, this is not always the case in Forex market data, as has been well stablished in literature. The correct way is to re-use parameters from the training set if we are doing any kind of transformation. So, the test set should basically stand for \"new, unseen\" data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train_std = sc.fit_transform(X_train)\n",
    "X_test_std = sc.transform(X_test)\n",
    "\n",
    "y_train_std = sc.fit_transform(y_train.reshape(-1, 1))\n",
    "y_test_std = sc.transform(y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.1, 1.7, 1.8, 1.8, 2.5, 2. , 1.9, 2.1, 2. , 2.4, 2.3, 1.8, 2.2,\n",
       "       2.3, 1.5, 2.3, 2. , 2. , 1.8, 2.1, 1.8, 1.8, 1.8, 2.1, 1.6, 1.9,\n",
       "       2. , 2.2, 1.5, 1.4, 2.3, 2.4, 1.8, 1.8, 2.1, 2.4, 2.3, 1.9, 2.3,\n",
       "       2.5, 2.3, 1.9, 2. , 2.3, 1.8])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.1],\n",
       "       [1.7],\n",
       "       [1.8],\n",
       "       [1.8],\n",
       "       [2.5],\n",
       "       [2. ],\n",
       "       [1.9],\n",
       "       [2.1],\n",
       "       [2. ],\n",
       "       [2.4],\n",
       "       [2.3],\n",
       "       [1.8],\n",
       "       [2.2],\n",
       "       [2.3],\n",
       "       [1.5],\n",
       "       [2.3],\n",
       "       [2. ],\n",
       "       [2. ],\n",
       "       [1.8],\n",
       "       [2.1],\n",
       "       [1.8],\n",
       "       [1.8],\n",
       "       [1.8],\n",
       "       [2.1],\n",
       "       [1.6],\n",
       "       [1.9],\n",
       "       [2. ],\n",
       "       [2.2],\n",
       "       [1.5],\n",
       "       [1.4],\n",
       "       [2.3],\n",
       "       [2.4],\n",
       "       [1.8],\n",
       "       [1.8],\n",
       "       [2.1],\n",
       "       [2.4],\n",
       "       [2.3],\n",
       "       [1.9],\n",
       "       [2.3],\n",
       "       [2.5],\n",
       "       [2.3],\n",
       "       [1.9],\n",
       "       [2. ],\n",
       "       [2.3],\n",
       "       [1.8]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.inverse_transform(y_test_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clears the default graph stack and resets the global default graph\n",
    "ops.reset_default_graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Placeholders\n",
      "Initializers\n"
     ]
    }
   ],
   "source": [
    "# make results reproducible\n",
    "seed = 2\n",
    "tf.set_random_seed(seed)\n",
    "np.random.seed(seed)  \n",
    "\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.005\n",
    "batch_size = 50\n",
    "n_features = X_train.shape[1]#  Number of features in training data\n",
    "epochs = 1000\n",
    "display_step = 50\n",
    "model_path = \"/tmp/model.ckpt\"\n",
    "n_classes = 1\n",
    "\n",
    "# Network Parameters\n",
    "# See figure of the model\n",
    "d0 = D = n_features # Layer 0 (Input layer number of features)\n",
    "d1 = 100 # Layer 1 (50 hidden nodes)\n",
    "d2 = 50 # Layer 2 (25 hidden nodes) \n",
    "d3 = 5 # Layer 3 (5 hidden nodes)\n",
    "d4 = C = 1 # Layer 4 (Output layer)\n",
    "\n",
    "# tf Graph input\n",
    "print(\"Placeholders\")\n",
    "X = tf.placeholder(dtype=tf.float32, shape=[None, n_features], name=\"X\")\n",
    "y = tf.placeholder(dtype=tf.float32, shape=[None,n_classes], name=\"y\")\n",
    "\n",
    "\n",
    "# Initializers\n",
    "print(\"Initializers\")\n",
    "sigma = 1\n",
    "weight_initializer = tf.variance_scaling_initializer(mode=\"fan_avg\", distribution=\"uniform\", scale=sigma)\n",
    "bias_initializer = tf.zeros_initializer()\n",
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(X, variables):\n",
    "    # Hidden layer with ReLU activation\n",
    "    layer_1 = tf.nn.relu(tf.add(tf.matmul(X, variables['W1']), variables['bias1']))\n",
    "    # Hidden layer with ReLU activation\n",
    "    layer_2 = tf.nn.relu(tf.add(tf.matmul(layer_1, variables['W2']), variables['bias2']))\n",
    "    # Hidden layer with ReLU activation\n",
    "    layer_3 = tf.nn.relu(tf.add(tf.matmul(layer_2, variables['W3']), variables['bias3']))\n",
    "    # Output layer with ReLU activation\n",
    "    out_layer = tf.nn.relu(tf.add(tf.matmul(layer_3, variables['W4']), variables['bias4']))\n",
    "    return out_layer\n",
    "\n",
    "# Store layers weight & bias\n",
    "variables = {\n",
    "    'W1': tf.Variable(weight_initializer([n_features, d1]), name=\"W1\"), # inputs -> d1 hidden neurons\n",
    "    'bias1': tf.Variable(bias_initializer([d1]), name=\"bias1\"), # one biases for each d1 hidden neurons\n",
    "    'W2': tf.Variable(weight_initializer([d1, d2]), name=\"W2\"), # d1 hidden inputs -> d2 hidden neurons\n",
    "    'bias2': tf.Variable(bias_initializer([d2]), name=\"bias2\"), # one biases for each d2 hidden neurons\n",
    "    'W3': tf.Variable(weight_initializer([d2, d3]), name=\"W3\"), ## d2 hidden inputs -> d3 hidden neurons\n",
    "    'bias3': tf.Variable(bias_initializer([d3]), name=\"bias3\"), # one biases for each d3 hidden neurons\n",
    "    'W4': tf.Variable(weight_initializer([d3, d4]), name=\"W4\"), # d3 hidden inputs -> 1 output\n",
    "    'bias4': tf.Variable(bias_initializer([d4]), name=\"bias4\") # 1 bias for the output\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "y_hat = multilayer_perceptron(X, variables)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss = tf.reduce_mean(tf.square(y - y_hat)) # MSE\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss) # Train step\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 'Saver' op to save and restore all the variables\n",
    "saver = tf.train.Saver()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model  and Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 1st session...\n",
      "Epoch: 0050 Lost= 0.121212967\n",
      "Epoch: 0100 Lost= 0.087612927\n",
      "Epoch: 0150 Lost= 0.054805990\n",
      "Epoch: 0200 Lost= 0.042091131\n",
      "Epoch: 0250 Lost= 0.033010442\n",
      "Epoch: 0300 Lost= 0.028964423\n",
      "Epoch: 0350 Lost= 0.033457872\n",
      "Epoch: 0400 Lost= 0.024154516\n",
      "Epoch: 0450 Lost= 0.033675905\n",
      "Epoch: 0500 Lost= 0.020445058\n",
      "Epoch: 0550 Lost= 0.022125030\n",
      "Epoch: 0600 Lost= 0.015964977\n",
      "Epoch: 0650 Lost= 0.014663436\n",
      "Epoch: 0700 Lost= 0.011865783\n",
      "Epoch: 0750 Lost= 0.014667089\n",
      "Epoch: 0800 Lost= 0.018193373\n",
      "Epoch: 0850 Lost= 0.014791524\n",
      "Epoch: 0900 Lost= 0.012803431\n",
      "Epoch: 0950 Lost= 0.011303975\n",
      "Epoch: 1000 Lost= 0.011635998\n",
      "Model saved in file: /tmp/model.ckpt\n",
      "First Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# Running first session\n",
    "print(\"Starting 1st session...\")\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Writer to record image, scalar, histogram and graph for display in tensorboard\n",
    "    writer = tf.summary.FileWriter(\"/tmp/tensorflow_logs\", sess.graph)  # create writer\n",
    "    writer.add_graph(sess.graph)\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        rand_index = np.random.choice(len(X_train_std), size=batch_size)\n",
    "        X_rand = X_train_std[rand_index]\n",
    "        y_rand = np.transpose([y_train[rand_index]])\n",
    "        sess.run(optimizer, feed_dict={X: X_rand, y: y_rand})\n",
    "\n",
    "        train_temp_loss = sess.run(loss, feed_dict={X: X_rand, y: y_rand})\n",
    "        train_loss.append(np.sqrt(train_temp_loss))\n",
    "    \n",
    "        test_temp_loss = sess.run(loss, feed_dict={X: X_test_std, y: np.transpose([y_test])})\n",
    "        test_loss.append(np.sqrt(test_temp_loss))\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"Lost=\", \\\n",
    "                \"{:.9f}\".format(train_temp_loss))\n",
    "\n",
    "    # Close writer\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "        \n",
    "    # Save model weights to disk\n",
    "    save_path = saver.save(sess, model_path)\n",
    "    print(\"Model saved in file: %s\" % save_path)\n",
    "    print(\"First Optimization Finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "# Plot loss (MSE) over time\n",
    "plt.plot(train_loss, 'k-', label='Train Loss')\n",
    "plt.plot(test_loss, 'r--', label='Test Loss')\n",
    "plt.title('Loss (MSE) per Generation')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard Graph\n",
    "\n",
    "\n",
    "What follows is the graph we have executed and all data about it. Note the \"save\" label and the several layers.\n",
    "\n",
    "\n",
    "![graph_4](../images/graph_4.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving a Tensorflow model\n",
    "\n",
    "So, now we have our model saved.\n",
    "\n",
    "Tensorflow model has four main files:\n",
    "* a) Meta graph:\n",
    "This is a protocol buffer which saves the complete Tensorflow graph; i.e. all variables, operations, collections etc. This file has .meta extension.\n",
    "\n",
    "\n",
    "* b) y c) Checkpoint files:\n",
    "It is a binary file which contains all the values of the weights, biases, gradients and all the other variables saved. Tensorflow has changed from version 0.11. Instead of a single .ckpt file, we have now two files: .index and .data file that contains our training variables. \n",
    "\n",
    "\n",
    "* d) Along with this, Tensorflow also has a file named checkpoint which simply keeps a record of latest checkpoint files saved.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain the model\n",
    "\n",
    "\n",
    "We can retrain the model as many times as we want to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 2nd session...\n",
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n",
      "Model restored from file: /tmp/model.ckpt\n",
      "Epoch: 0050 Lost= 0.016628481\n",
      "Epoch: 0100 Lost= 0.011171184\n",
      "Epoch: 0150 Lost= 0.014480609\n",
      "Epoch: 0200 Lost= 0.010597562\n",
      "Epoch: 0250 Lost= 0.009532723\n",
      "Epoch: 0300 Lost= 0.010108640\n",
      "Epoch: 0350 Lost= 0.009826462\n",
      "Epoch: 0400 Lost= 0.013368943\n",
      "Epoch: 0450 Lost= 0.010665327\n",
      "Epoch: 0500 Lost= 0.008773899\n",
      "Epoch: 0550 Lost= 0.008465427\n",
      "Epoch: 0600 Lost= 0.013776772\n",
      "Epoch: 0650 Lost= 0.011079680\n",
      "Epoch: 0700 Lost= 0.011942017\n",
      "Epoch: 0750 Lost= 0.013662893\n",
      "Epoch: 0800 Lost= 0.010473980\n",
      "Epoch: 0850 Lost= 0.013280788\n",
      "Epoch: 0900 Lost= 0.013363536\n",
      "Epoch: 0950 Lost= 0.012372569\n",
      "Epoch: 1000 Lost= 0.014634406\n",
      "Model saved in file: /tmp/model.ckpt\n",
      "Second Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# Running a new session\n",
    "print(\"Starting 2nd session...\")\n",
    "with tf.Session() as sess:\n",
    "    # Initialize variables\n",
    "    sess.run(init)\n",
    "\n",
    "    # Restore model weights from previously saved model\n",
    "    saver.restore(sess, model_path)\n",
    "    print(\"Model restored from file: %s\" % model_path)\n",
    "\n",
    "    # Resume training\n",
    "    for epoch in range(epochs):\n",
    "        rand_index = np.random.choice(len(X_train), size=batch_size)\n",
    "        X_rand = X_train_std[rand_index]\n",
    "        y_rand = np.transpose([y_train[rand_index]])\n",
    "        sess.run(optimizer, feed_dict={X: X_rand, y: y_rand})\n",
    "\n",
    "        train_temp_loss = sess.run(loss, feed_dict={X: X_rand, y: y_rand})\n",
    "        train_loss.append(np.sqrt(train_temp_loss))\n",
    "    \n",
    "        test_temp_loss = sess.run(loss, feed_dict={X: X_test_std, y: np.transpose([y_test])})\n",
    "        test_loss.append(np.sqrt(test_temp_loss))\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"Lost=\", \\\n",
    "                \"{:.9f}\".format(train_temp_loss))\n",
    "\n",
    "    # Close writer\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "    \n",
    "    # Save model weights to disk\n",
    "    save_path = saver.save(sess, model_path)\n",
    "    print(\"Model saved in file: %s\" % save_path)\n",
    "    print(\"Second Optimization Finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.2, 0.1,\n",
       "       0.1, 0.2, 0.4, 0.4, 0.3, 0.3, 0.3, 0.2, 0.4, 0.2, 0.5, 0.2, 0.2,\n",
       "       0.4, 0.2, 0.2, 0.2, 0.2, 0.4, 0.1, 0.2, 0.1, 0.2, 0.2, 0.1, 0.2,\n",
       "       0.2, 0.3, 0.3, 0.2, 0.6, 0.4, 0.3, 0.2, 0.2, 0.2, 0.2, 1.4, 1.5,\n",
       "       1.5, 1.3, 1.5, 1.3, 1.6, 1. , 1.3, 1.4, 1. , 1.5, 1. , 1.4, 1.3,\n",
       "       1.4, 1.5, 1. , 1.5, 1.1, 1.8, 1.3, 1.5, 1.2, 1.3, 1.4, 1.4, 1.7,\n",
       "       1.5, 1. , 1.1, 1. , 1.2, 1.6, 1.5, 1.6, 1.5, 1.3, 1.3, 1.3, 1.2,\n",
       "       1.4, 1.2, 1. , 1.3, 1.2, 1.3, 1.3, 1.1, 1.3, 2.5, 1.9, 2.1, 1.8,\n",
       "       2.2, 2.1, 1.7, 1.8, 1.8, 2.5, 2. , 1.9, 2.1, 2. , 2.4, 2.3, 1.8,\n",
       "       2.2, 2.3, 1.5, 2.3, 2. , 2. , 1.8, 2.1, 1.8, 1.8, 1.8, 2.1, 1.6,\n",
       "       1.9, 2. , 2.2, 1.5, 1.4, 2.3, 2.4, 1.8, 1.8, 2.1, 2.4, 2.3, 1.9,\n",
       "       2.3, 2.5, 2.3, 1.9, 2. , 2.3, 1.8])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use the model to make some predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prediction session...\n",
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n",
      "Model restored from file: /tmp/model.ckpt\n",
      "[[2.5972207]\n",
      " [2.4428723]\n",
      " [5.2549114]]\n"
     ]
    }
   ],
   "source": [
    "# Running a new session for predictions\n",
    "print(\"Starting prediction session...\")\n",
    "with tf.Session() as sess:\n",
    "    # Initialize variables\n",
    "    sess.run(init)\n",
    "\n",
    "    # Restore model weights from previously saved model\n",
    "    saver.restore(sess, model_path)\n",
    "    print(\"Model restored from file: %s\" % model_path)\n",
    "\n",
    "    # We try to predict the petal width (cm) of three samples\n",
    "    feed_dict = {X: [[5.1, 3.5, 1.4],\n",
    "                     [4.8, 3.0, 1.4],\n",
    "                     [6.3, 3.4, 5.6]]\n",
    "                }\n",
    "    prediction = sess.run(y_hat, feed_dict)\n",
    "    print(prediction) # True value 0.2, 0.1, 2.4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, better results, but still not very good results. We could try to improve them with a deeper network (more layers) or retouching the net parameters and number of neurons. That is another story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prediction session...\n",
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n",
      "Model restored from file: /tmp/model.ckpt\n",
      "[[2.7591481]\n",
      " [2.5216594]\n",
      " [6.7912245]]\n"
     ]
    }
   ],
   "source": [
    "# Running a new session for predictions\n",
    "print(\"Starting prediction session...\")\n",
    "with tf.Session() as sess:\n",
    "    # Initialize variables\n",
    "    sess.run(init)\n",
    "\n",
    "    # Restore model weights from previously saved model\n",
    "    saver.restore(sess, model_path)\n",
    "    print(\"Model restored from file: %s\" % model_path)\n",
    "\n",
    "    # We try to predict the petal width (cm) of three samples\n",
    "    feed_dict_std = {X: [[6.86549436, 4.28228483, 0.89182234],\n",
    "       [6.38114257, 3.47503186, 0.89182234],\n",
    "       [8.8029015 , 4.12083424, 7.67274733]]}\n",
    "    prediction = sess.run(y_hat, feed_dict_std)\n",
    "    print(prediction) # True value 0.2, 0.1, 2.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.5565927],\n",
       "       [2.4094958],\n",
       "       [5.0539985]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat_rev = sc.inverse_transform(prediction)\n",
    "y_hat_rev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not good. We'll see later why."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "deeptrading",
   "language": "python",
   "name": "deeptrading"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
