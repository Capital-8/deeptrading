{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a multiple hidden layer Neural Network \n",
    "\n",
    "\n",
    "![deep_network_model](../images/deep_network_model.png)\n",
    "\n",
    "\n",
    "The progress of the model can be saved during and after training. This means that a model can be resumed where it left off and avoid long training times. Saving also means that you can share your model and others can recreate your work.\n",
    "\n",
    "We will illustrate how to create a multiple fully connected hidden layer NN, save it and make predictions with trained model after reload it.\n",
    "\n",
    "We will use the iris data for this exercise.\n",
    "\n",
    "We will build a three-hidden layer neural network  to predict the fourth attribute, Petal Width from the other three (Sepal length, Sepal width, Petal length)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parrondo/anaconda3/envs/deeptrading/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/parrondo/anaconda3/envs/deeptrading/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/parrondo/anaconda3/envs/deeptrading/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/parrondo/anaconda3/envs/deeptrading/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_iris\n",
    "from tensorflow.python.framework import ops\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of dataset\n",
      "n= 150 p= 3\n"
     ]
    }
   ],
   "source": [
    "# Before getting into pandas dataframes we will load an example dataset from sklearn library \n",
    "# type(data) #iris is a bunch instance which is inherited from dictionary\n",
    "data = load_iris() #load iris dataset\n",
    "\n",
    "# We get a pandas dataframe to better visualize the datasets\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "\n",
    "X_raw = np.array([x[0:3] for x in data.data])\n",
    "y_raw = np.array([x[3] for x in data.data])\n",
    "\n",
    "# Dimensions of dataset\n",
    "print(\"Dimensions of dataset\")\n",
    "n = X_raw.shape[0]\n",
    "p = X_raw.shape[1]\n",
    "print(\"n=\",n,\"p=\",p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['DESCR', 'data', 'target', 'target_names', 'feature_names'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys() #keys of the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_raw.shape # Array 150x3. Each element is a 3-dimensional data point: sepal length, sepal width, petal length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_raw.shape # Vector 150. Each element is a 1-dimensional (scalar) data point: petal width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.7</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>5.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.9</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>6.2</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>6.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.8</td>\n",
       "      <td>1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>7.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.1</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>7.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>6.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>6.4</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>6.8</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.9</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                  5.1               3.5                1.4               0.2\n",
       "1                  4.9               3.0                1.4               0.2\n",
       "2                  4.7               3.2                1.3               0.2\n",
       "3                  4.6               3.1                1.5               0.2\n",
       "4                  5.0               3.6                1.4               0.2\n",
       "5                  5.4               3.9                1.7               0.4\n",
       "6                  4.6               3.4                1.4               0.3\n",
       "7                  5.0               3.4                1.5               0.2\n",
       "8                  4.4               2.9                1.4               0.2\n",
       "9                  4.9               3.1                1.5               0.1\n",
       "10                 5.4               3.7                1.5               0.2\n",
       "11                 4.8               3.4                1.6               0.2\n",
       "12                 4.8               3.0                1.4               0.1\n",
       "13                 4.3               3.0                1.1               0.1\n",
       "14                 5.8               4.0                1.2               0.2\n",
       "15                 5.7               4.4                1.5               0.4\n",
       "16                 5.4               3.9                1.3               0.4\n",
       "17                 5.1               3.5                1.4               0.3\n",
       "18                 5.7               3.8                1.7               0.3\n",
       "19                 5.1               3.8                1.5               0.3\n",
       "20                 5.4               3.4                1.7               0.2\n",
       "21                 5.1               3.7                1.5               0.4\n",
       "22                 4.6               3.6                1.0               0.2\n",
       "23                 5.1               3.3                1.7               0.5\n",
       "24                 4.8               3.4                1.9               0.2\n",
       "25                 5.0               3.0                1.6               0.2\n",
       "26                 5.0               3.4                1.6               0.4\n",
       "27                 5.2               3.5                1.5               0.2\n",
       "28                 5.2               3.4                1.4               0.2\n",
       "29                 4.7               3.2                1.6               0.2\n",
       "..                 ...               ...                ...               ...\n",
       "120                6.9               3.2                5.7               2.3\n",
       "121                5.6               2.8                4.9               2.0\n",
       "122                7.7               2.8                6.7               2.0\n",
       "123                6.3               2.7                4.9               1.8\n",
       "124                6.7               3.3                5.7               2.1\n",
       "125                7.2               3.2                6.0               1.8\n",
       "126                6.2               2.8                4.8               1.8\n",
       "127                6.1               3.0                4.9               1.8\n",
       "128                6.4               2.8                5.6               2.1\n",
       "129                7.2               3.0                5.8               1.6\n",
       "130                7.4               2.8                6.1               1.9\n",
       "131                7.9               3.8                6.4               2.0\n",
       "132                6.4               2.8                5.6               2.2\n",
       "133                6.3               2.8                5.1               1.5\n",
       "134                6.1               2.6                5.6               1.4\n",
       "135                7.7               3.0                6.1               2.3\n",
       "136                6.3               3.4                5.6               2.4\n",
       "137                6.4               3.1                5.5               1.8\n",
       "138                6.0               3.0                4.8               1.8\n",
       "139                6.9               3.1                5.4               2.1\n",
       "140                6.7               3.1                5.6               2.4\n",
       "141                6.9               3.1                5.1               2.3\n",
       "142                5.8               2.7                5.1               1.9\n",
       "143                6.8               3.2                5.9               2.3\n",
       "144                6.7               3.3                5.7               2.5\n",
       "145                6.7               3.0                5.2               2.3\n",
       "146                6.3               2.5                5.0               1.9\n",
       "147                6.5               3.0                5.2               2.0\n",
       "148                6.2               3.4                5.4               2.3\n",
       "149                5.9               3.0                5.1               1.8\n",
       "\n",
       "[150 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Leave in blanck intentionally\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples:  150 \n",
      "Samples in train set:  105 \n",
      "Samples in test set:  45\n",
      "X_train.shape =  (105, 3) y_train.shape = (105,) \n",
      "X_test.shape =   (45, 3) y_test.shape =  (45,)\n"
     ]
    }
   ],
   "source": [
    "# split into train and test sets\n",
    "\n",
    "# Total samples\n",
    "nsamples = n\n",
    "\n",
    "# Splitting into train (70%) and test (30%) sets\n",
    "split = 70 # training split% ; test (100-split)%\n",
    "jindex = nsamples*split//100 # Index for slicing the samples\n",
    "\n",
    "# Samples in train\n",
    "nsamples_train = jindex\n",
    "\n",
    "# Samples in test\n",
    "nsamples_test = nsamples - nsamples_train\n",
    "print(\"Total number of samples: \",nsamples,\"\\nSamples in train set: \", nsamples_train,\n",
    "      \"\\nSamples in test set: \",nsamples_test)\n",
    "\n",
    "# Here are train and test samples\n",
    "X_train = X_raw[:jindex, :]\n",
    "y_train = y_raw[:jindex]\n",
    "\n",
    "X_test = X_raw[jindex:, :]\n",
    "y_test = y_raw[jindex:]\n",
    "\n",
    "print(\"X_train.shape = \", X_train.shape, \"y_train.shape =\", y_train.shape, \"\\nX_test.shape =  \",\n",
    "      X_test.shape, \"y_test.shape = \", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**\n",
    "\n",
    "Becareful not to write `X_test_std = sc.fit_transform(X_test)` instead of `X_test_std = sc.transform(X_test)`. In this case, it wouldn't make a great difference since the mean and standard deviation of the test set should be (quite) similar to the training set. However, this is not always the case in Forex market data, as has been well stablished in literature. The correct way is to re-use parameters from the training set if we are doing any kind of transformation. So, the test set should basically stand for \"new, unseen\" data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train_std = sc.fit_transform(X_train)\n",
    "X_test_std = sc.transform(X_test)\n",
    "\n",
    "y_train_std = sc.fit_transform(y_train.reshape(-1, 1))\n",
    "y_test_std = sc.transform(y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clears the default graph stack and resets the global default graph\n",
    "ops.reset_default_graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Placeholders\n",
      "Initializers\n"
     ]
    }
   ],
   "source": [
    "# make results reproducible\n",
    "seed = 2\n",
    "tf.set_random_seed(seed)\n",
    "np.random.seed(seed)  \n",
    "\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.005\n",
    "batch_size = 50\n",
    "n_features = X_train.shape[1]#  Number of features in training data\n",
    "epochs = 1000*10\n",
    "display_step = 50\n",
    "model_path = \"/tmp/model.ckpt\"\n",
    "n_classes = 1\n",
    "\n",
    "# Network Parameters\n",
    "# See figure of the model\n",
    "d0 = D = n_features # Layer 0 (Input layer number of features)\n",
    "d1 = 64 # Layer 1 (50 hidden nodes)\n",
    "d2 = 32 # Layer 2 (25 hidden nodes) \n",
    "d3 = 8 # Layer 3 (5 hidden nodes)\n",
    "d4 = C = 1 # Layer 4 (Output layer)\n",
    "\n",
    "# tf Graph input\n",
    "print(\"Placeholders\")\n",
    "X = tf.placeholder(dtype=tf.float32, shape=[None, n_features], name=\"X\")\n",
    "y = tf.placeholder(dtype=tf.float32, shape=[None,n_classes], name=\"y\")\n",
    "\n",
    "\n",
    "# Initializers\n",
    "print(\"Initializers\")\n",
    "sigma = 1\n",
    "weight_initializer = tf.variance_scaling_initializer(mode=\"fan_avg\", distribution=\"uniform\", scale=sigma)\n",
    "bias_initializer = tf.zeros_initializer()\n",
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(X, variables):\n",
    "    # Hidden layer with ReLU activation\n",
    "    layer_1 = tf.nn.relu(tf.add(tf.matmul(X, variables['W1']), variables['bias1']))\n",
    "    # Hidden layer with ReLU activation\n",
    "    layer_2 = tf.nn.relu(tf.add(tf.matmul(layer_1, variables['W2']), variables['bias2']))\n",
    "    # Hidden layer with ReLU activation\n",
    "    layer_3 = tf.nn.relu(tf.add(tf.matmul(layer_2, variables['W3']), variables['bias3']))\n",
    "    # Output layer with ReLU activation\n",
    "    out_layer = tf.nn.relu(tf.add(tf.matmul(layer_3, variables['W4']), variables['bias4']))\n",
    "    return out_layer\n",
    "\n",
    "# Store layers weight & bias\n",
    "variables = {\n",
    "    'W1': tf.Variable(weight_initializer([n_features, d1]), name=\"W1\"), # inputs -> d1 hidden neurons\n",
    "    'bias1': tf.Variable(bias_initializer([d1]), name=\"bias1\"), # one biases for each d1 hidden neurons\n",
    "    'W2': tf.Variable(weight_initializer([d1, d2]), name=\"W2\"), # d1 hidden inputs -> d2 hidden neurons\n",
    "    'bias2': tf.Variable(bias_initializer([d2]), name=\"bias2\"), # one biases for each d2 hidden neurons\n",
    "    'W3': tf.Variable(weight_initializer([d2, d3]), name=\"W3\"), ## d2 hidden inputs -> d3 hidden neurons\n",
    "    'bias3': tf.Variable(bias_initializer([d3]), name=\"bias3\"), # one biases for each d3 hidden neurons\n",
    "    'W4': tf.Variable(weight_initializer([d3, d4]), name=\"W4\"), # d3 hidden inputs -> 1 output\n",
    "    'bias4': tf.Variable(bias_initializer([d4]), name=\"bias4\") # 1 bias for the output\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "y_hat = multilayer_perceptron(X, variables)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss = tf.reduce_mean(tf.square(y - y_hat)) # MSE\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss) # Train step\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 'Saver' op to save and restore all the variables\n",
    "saver = tf.train.Saver()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model  and Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 1st session...\n",
      "Epoch: 0050 Lost= 0.086277314\n",
      "Epoch: 0100 Lost= 0.051621340\n",
      "Epoch: 0150 Lost= 0.039545797\n",
      "Epoch: 0200 Lost= 0.024052400\n",
      "Epoch: 0250 Lost= 0.025915693\n",
      "Epoch: 0300 Lost= 0.022879438\n",
      "Epoch: 0350 Lost= 0.027491007\n",
      "Epoch: 0400 Lost= 0.022818303\n",
      "Epoch: 0450 Lost= 0.040130019\n",
      "Epoch: 0500 Lost= 0.018311702\n",
      "Epoch: 0550 Lost= 0.018461309\n",
      "Epoch: 0600 Lost= 0.017378828\n",
      "Epoch: 0650 Lost= 0.023035271\n",
      "Epoch: 0700 Lost= 0.013548579\n",
      "Epoch: 0750 Lost= 0.016563468\n",
      "Epoch: 0800 Lost= 0.021350224\n",
      "Epoch: 0850 Lost= 0.017457008\n",
      "Epoch: 0900 Lost= 0.018559664\n",
      "Epoch: 0950 Lost= 0.012122941\n",
      "Epoch: 1000 Lost= 0.014214334\n",
      "Epoch: 1050 Lost= 0.021690818\n",
      "Epoch: 1100 Lost= 0.014668341\n",
      "Epoch: 1150 Lost= 0.016625326\n",
      "Epoch: 1200 Lost= 0.014646606\n",
      "Epoch: 1250 Lost= 0.009548829\n",
      "Epoch: 1300 Lost= 0.011772913\n",
      "Epoch: 1350 Lost= 0.010223914\n",
      "Epoch: 1400 Lost= 0.015687490\n",
      "Epoch: 1450 Lost= 0.012099712\n",
      "Epoch: 1500 Lost= 0.011584572\n",
      "Epoch: 1550 Lost= 0.009453238\n",
      "Epoch: 1600 Lost= 0.012440037\n",
      "Epoch: 1650 Lost= 0.011573870\n",
      "Epoch: 1700 Lost= 0.011060100\n",
      "Epoch: 1750 Lost= 0.015413016\n",
      "Epoch: 1800 Lost= 0.012822747\n",
      "Epoch: 1850 Lost= 0.017076623\n",
      "Epoch: 1900 Lost= 0.015857108\n",
      "Epoch: 1950 Lost= 0.018535525\n",
      "Epoch: 2000 Lost= 0.015850745\n",
      "Epoch: 2050 Lost= 0.009969752\n",
      "Epoch: 2100 Lost= 0.008313730\n",
      "Epoch: 2150 Lost= 0.011913124\n",
      "Epoch: 2200 Lost= 0.010079362\n",
      "Epoch: 2250 Lost= 0.010006999\n",
      "Epoch: 2300 Lost= 0.012005989\n",
      "Epoch: 2350 Lost= 0.007795457\n",
      "Epoch: 2400 Lost= 0.013089273\n",
      "Epoch: 2450 Lost= 0.012068433\n",
      "Epoch: 2500 Lost= 0.010614517\n",
      "Epoch: 2550 Lost= 0.017531468\n",
      "Epoch: 2600 Lost= 0.010183845\n",
      "Epoch: 2650 Lost= 0.011182955\n",
      "Epoch: 2700 Lost= 0.010032325\n",
      "Epoch: 2750 Lost= 0.008830270\n",
      "Epoch: 2800 Lost= 0.008528909\n",
      "Epoch: 2850 Lost= 0.010154128\n",
      "Epoch: 2900 Lost= 0.008674047\n",
      "Epoch: 2950 Lost= 0.008348025\n",
      "Epoch: 3000 Lost= 0.011660291\n",
      "Epoch: 3050 Lost= 0.010719614\n",
      "Epoch: 3100 Lost= 0.010725587\n",
      "Epoch: 3150 Lost= 0.013605397\n",
      "Epoch: 3200 Lost= 0.009840936\n",
      "Epoch: 3250 Lost= 0.010346905\n",
      "Epoch: 3300 Lost= 0.006851782\n",
      "Epoch: 3350 Lost= 0.011763534\n",
      "Epoch: 3400 Lost= 0.008203003\n",
      "Epoch: 3450 Lost= 0.007936621\n",
      "Epoch: 3500 Lost= 0.009458246\n",
      "Epoch: 3550 Lost= 0.008350388\n",
      "Epoch: 3600 Lost= 0.012163502\n",
      "Epoch: 3650 Lost= 0.007014058\n",
      "Epoch: 3700 Lost= 0.010694268\n",
      "Epoch: 3750 Lost= 0.011672390\n",
      "Epoch: 3800 Lost= 0.013256729\n",
      "Epoch: 3850 Lost= 0.008550898\n",
      "Epoch: 3900 Lost= 0.008993178\n",
      "Epoch: 3950 Lost= 0.012192531\n",
      "Epoch: 4000 Lost= 0.007848776\n",
      "Epoch: 4050 Lost= 0.008796414\n",
      "Epoch: 4100 Lost= 0.011429868\n",
      "Epoch: 4150 Lost= 0.010912986\n",
      "Epoch: 4200 Lost= 0.008544314\n",
      "Epoch: 4250 Lost= 0.015417440\n",
      "Epoch: 4300 Lost= 0.007145215\n",
      "Epoch: 4350 Lost= 0.010296518\n",
      "Epoch: 4400 Lost= 0.006762174\n",
      "Epoch: 4450 Lost= 0.006884578\n",
      "Epoch: 4500 Lost= 0.008637001\n",
      "Epoch: 4550 Lost= 0.011556608\n",
      "Epoch: 4600 Lost= 0.011489591\n",
      "Epoch: 4650 Lost= 0.007845979\n",
      "Epoch: 4700 Lost= 0.007762603\n",
      "Epoch: 4750 Lost= 0.010583572\n",
      "Epoch: 4800 Lost= 0.009106914\n",
      "Epoch: 4850 Lost= 0.010296550\n",
      "Epoch: 4900 Lost= 0.008003155\n",
      "Epoch: 4950 Lost= 0.011024765\n",
      "Epoch: 5000 Lost= 0.007461882\n",
      "Epoch: 5050 Lost= 0.013462221\n",
      "Epoch: 5100 Lost= 0.008752324\n",
      "Epoch: 5150 Lost= 0.009146453\n",
      "Epoch: 5200 Lost= 0.010519649\n",
      "Epoch: 5250 Lost= 0.006201196\n",
      "Epoch: 5300 Lost= 0.008477719\n",
      "Epoch: 5350 Lost= 0.011258371\n",
      "Epoch: 5400 Lost= 0.012928883\n",
      "Epoch: 5450 Lost= 0.008059479\n",
      "Epoch: 5500 Lost= 0.009079533\n",
      "Epoch: 5550 Lost= 0.010747364\n",
      "Epoch: 5600 Lost= 0.009776867\n",
      "Epoch: 5650 Lost= 0.013761509\n",
      "Epoch: 5700 Lost= 0.007751191\n",
      "Epoch: 5750 Lost= 0.007688271\n",
      "Epoch: 5800 Lost= 0.009192056\n",
      "Epoch: 5850 Lost= 0.013921401\n",
      "Epoch: 5900 Lost= 0.011340418\n",
      "Epoch: 5950 Lost= 0.007768742\n",
      "Epoch: 6000 Lost= 0.008650796\n",
      "Epoch: 6050 Lost= 0.008627801\n",
      "Epoch: 6100 Lost= 0.005598166\n",
      "Epoch: 6150 Lost= 0.007933405\n",
      "Epoch: 6200 Lost= 0.009602344\n",
      "Epoch: 6250 Lost= 0.012294007\n",
      "Epoch: 6300 Lost= 0.010070436\n",
      "Epoch: 6350 Lost= 0.008960366\n",
      "Epoch: 6400 Lost= 0.007538636\n",
      "Epoch: 6450 Lost= 0.006735409\n",
      "Epoch: 6500 Lost= 0.006468700\n",
      "Epoch: 6550 Lost= 0.006984168\n",
      "Epoch: 6600 Lost= 0.009629623\n",
      "Epoch: 6650 Lost= 0.006966682\n",
      "Epoch: 6700 Lost= 0.007946333\n",
      "Epoch: 6750 Lost= 0.007569656\n",
      "Epoch: 6800 Lost= 0.009293241\n",
      "Epoch: 6850 Lost= 0.012435266\n",
      "Epoch: 6900 Lost= 0.010712732\n",
      "Epoch: 6950 Lost= 0.004563572\n",
      "Epoch: 7000 Lost= 0.007265460\n",
      "Epoch: 7050 Lost= 0.011766672\n",
      "Epoch: 7100 Lost= 0.009849546\n",
      "Epoch: 7150 Lost= 0.008946939\n",
      "Epoch: 7200 Lost= 0.009193098\n",
      "Epoch: 7250 Lost= 0.009210784\n",
      "Epoch: 7300 Lost= 0.009063322\n",
      "Epoch: 7350 Lost= 0.007132773\n",
      "Epoch: 7400 Lost= 0.006333557\n",
      "Epoch: 7450 Lost= 0.008721298\n",
      "Epoch: 7500 Lost= 0.009715842\n",
      "Epoch: 7550 Lost= 0.007352191\n",
      "Epoch: 7600 Lost= 0.007626357\n",
      "Epoch: 7650 Lost= 0.010158621\n",
      "Epoch: 7700 Lost= 0.008336757\n",
      "Epoch: 7750 Lost= 0.009431298\n",
      "Epoch: 7800 Lost= 0.009392386\n",
      "Epoch: 7850 Lost= 0.006957678\n",
      "Epoch: 7900 Lost= 0.008543933\n",
      "Epoch: 7950 Lost= 0.008790937\n",
      "Epoch: 8000 Lost= 0.010058511\n",
      "Epoch: 8050 Lost= 0.010024234\n",
      "Epoch: 8100 Lost= 0.009406555\n",
      "Epoch: 8150 Lost= 0.007698987\n",
      "Epoch: 8200 Lost= 0.011016645\n",
      "Epoch: 8250 Lost= 0.007867399\n",
      "Epoch: 8300 Lost= 0.006759791\n",
      "Epoch: 8350 Lost= 0.011109227\n",
      "Epoch: 8400 Lost= 0.003540111\n",
      "Epoch: 8450 Lost= 0.010893278\n",
      "Epoch: 8500 Lost= 0.005056000\n",
      "Epoch: 8550 Lost= 0.011923965\n",
      "Epoch: 8600 Lost= 0.010874979\n",
      "Epoch: 8650 Lost= 0.007275675\n",
      "Epoch: 8700 Lost= 0.008727899\n",
      "Epoch: 8750 Lost= 0.009318679\n",
      "Epoch: 8800 Lost= 0.007832658\n",
      "Epoch: 8850 Lost= 0.010091549\n",
      "Epoch: 8900 Lost= 0.010293987\n",
      "Epoch: 8950 Lost= 0.007727380\n",
      "Epoch: 9000 Lost= 0.009990769\n",
      "Epoch: 9050 Lost= 0.010642886\n",
      "Epoch: 9100 Lost= 0.008251244\n",
      "Epoch: 9150 Lost= 0.011891425\n",
      "Epoch: 9200 Lost= 0.008378832\n",
      "Epoch: 9250 Lost= 0.007472759\n",
      "Epoch: 9300 Lost= 0.007888145\n",
      "Epoch: 9350 Lost= 0.010437286\n",
      "Epoch: 9400 Lost= 0.007591054\n",
      "Epoch: 9450 Lost= 0.007948128\n",
      "Epoch: 9500 Lost= 0.008549827\n",
      "Epoch: 9550 Lost= 0.006314007\n",
      "Epoch: 9600 Lost= 0.010670049\n",
      "Epoch: 9650 Lost= 0.008298525\n",
      "Epoch: 9700 Lost= 0.007582198\n",
      "Epoch: 9750 Lost= 0.008822577\n",
      "Epoch: 9800 Lost= 0.007767251\n",
      "Epoch: 9850 Lost= 0.007024411\n",
      "Epoch: 9900 Lost= 0.009337789\n",
      "Epoch: 9950 Lost= 0.012272637\n",
      "Epoch: 10000 Lost= 0.009887830\n",
      "Model saved in file: /tmp/model.ckpt\n",
      "First Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# Running first session\n",
    "print(\"Starting 1st session...\")\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Writer to record image, scalar, histogram and graph for display in tensorboard\n",
    "    writer = tf.summary.FileWriter(\"/tmp/tensorflow_logs\", sess.graph)  # create writer\n",
    "    writer.add_graph(sess.graph)\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        rand_index = np.random.choice(len(X_train_std), size=batch_size)\n",
    "        X_rand = X_train_std[rand_index]\n",
    "        y_rand = np.transpose([y_train[rand_index]])\n",
    "        sess.run(optimizer, feed_dict={X: X_rand, y: y_rand})\n",
    "\n",
    "        train_temp_loss = sess.run(loss, feed_dict={X: X_rand, y: y_rand})\n",
    "        train_loss.append(np.sqrt(train_temp_loss))\n",
    "    \n",
    "        test_temp_loss = sess.run(loss, feed_dict={X: X_test_std, y: np.transpose([y_test])})\n",
    "        test_loss.append(np.sqrt(test_temp_loss))\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"Lost=\", \\\n",
    "                \"{:.9f}\".format(train_temp_loss))\n",
    "\n",
    "    # Close writer\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "        \n",
    "    # Save model weights to disk\n",
    "    save_path = saver.save(sess, model_path)\n",
    "    print(\"Model saved in file: %s\" % save_path)\n",
    "    print(\"First Optimization Finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "# Plot loss (MSE) over time\n",
    "plt.plot(train_loss, 'k-', label='Train Loss')\n",
    "plt.plot(test_loss, 'r--', label='Test Loss')\n",
    "plt.title('Loss (MSE) per Generation')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard Graph\n",
    "\n",
    "\n",
    "What follows is the graph we have executed and all data about it. Note the \"save\" label and the several layers.\n",
    "\n",
    "\n",
    "![graph_4](../images/graph_4.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving a Tensorflow model\n",
    "\n",
    "So, now we have our model saved.\n",
    "\n",
    "Tensorflow model has four main files:\n",
    "* a) Meta graph:\n",
    "This is a protocol buffer which saves the complete Tensorflow graph; i.e. all variables, operations, collections etc. This file has .meta extension.\n",
    "\n",
    "\n",
    "* b) y c) Checkpoint files:\n",
    "It is a binary file which contains all the values of the weights, biases, gradients and all the other variables saved. Tensorflow has changed from version 0.11. Instead of a single .ckpt file, we have now two files: .index and .data file that contains our training variables. \n",
    "\n",
    "\n",
    "* d) Along with this, Tensorflow also has a file named checkpoint which simply keeps a record of latest checkpoint files saved.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain the model\n",
    "\n",
    "\n",
    "We can retrain the model as many times as we want to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 2nd session...\n",
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n",
      "Model restored from file: /tmp/model.ckpt\n",
      "Epoch: 0050 Lost= 0.009576011\n",
      "Epoch: 0100 Lost= 0.011532238\n",
      "Epoch: 0150 Lost= 0.008338327\n",
      "Epoch: 0200 Lost= 0.012411817\n",
      "Epoch: 0250 Lost= 0.011600719\n",
      "Epoch: 0300 Lost= 0.010207349\n",
      "Epoch: 0350 Lost= 0.011961358\n",
      "Epoch: 0400 Lost= 0.007444927\n",
      "Epoch: 0450 Lost= 0.007741953\n",
      "Epoch: 0500 Lost= 0.009505201\n",
      "Epoch: 0550 Lost= 0.009177241\n",
      "Epoch: 0600 Lost= 0.006287719\n",
      "Epoch: 0650 Lost= 0.010511926\n",
      "Epoch: 0700 Lost= 0.007705266\n",
      "Epoch: 0750 Lost= 0.008198298\n",
      "Epoch: 0800 Lost= 0.011740243\n",
      "Epoch: 0850 Lost= 0.012126230\n",
      "Epoch: 0900 Lost= 0.008342671\n",
      "Epoch: 0950 Lost= 0.009153378\n",
      "Epoch: 1000 Lost= 0.009790989\n",
      "Epoch: 1050 Lost= 0.008468764\n",
      "Epoch: 1100 Lost= 0.008865799\n",
      "Epoch: 1150 Lost= 0.007013062\n",
      "Epoch: 1200 Lost= 0.005181016\n",
      "Epoch: 1250 Lost= 0.006817854\n",
      "Epoch: 1300 Lost= 0.009733246\n",
      "Epoch: 1350 Lost= 0.007313926\n",
      "Epoch: 1400 Lost= 0.008869968\n",
      "Epoch: 1450 Lost= 0.012473195\n",
      "Epoch: 1500 Lost= 0.008000494\n",
      "Epoch: 1550 Lost= 0.007868974\n",
      "Epoch: 1600 Lost= 0.009891218\n",
      "Epoch: 1650 Lost= 0.005326065\n",
      "Epoch: 1700 Lost= 0.008451692\n",
      "Epoch: 1750 Lost= 0.007923461\n",
      "Epoch: 1800 Lost= 0.009123000\n",
      "Epoch: 1850 Lost= 0.010021538\n",
      "Epoch: 1900 Lost= 0.010701101\n",
      "Epoch: 1950 Lost= 0.005630782\n",
      "Epoch: 2000 Lost= 0.007620377\n",
      "Epoch: 2050 Lost= 0.009025683\n",
      "Epoch: 2100 Lost= 0.011410631\n",
      "Epoch: 2150 Lost= 0.007817437\n",
      "Epoch: 2200 Lost= 0.008074515\n",
      "Epoch: 2250 Lost= 0.007915802\n",
      "Epoch: 2300 Lost= 0.010893787\n",
      "Epoch: 2350 Lost= 0.009539009\n",
      "Epoch: 2400 Lost= 0.007163433\n",
      "Epoch: 2450 Lost= 0.009548423\n",
      "Epoch: 2500 Lost= 0.006786576\n",
      "Epoch: 2550 Lost= 0.007907857\n",
      "Epoch: 2600 Lost= 0.010729308\n",
      "Epoch: 2650 Lost= 0.010231698\n",
      "Epoch: 2700 Lost= 0.006866338\n",
      "Epoch: 2750 Lost= 0.007241843\n",
      "Epoch: 2800 Lost= 0.008924846\n",
      "Epoch: 2850 Lost= 0.013424265\n",
      "Epoch: 2900 Lost= 0.008133926\n",
      "Epoch: 2950 Lost= 0.008997504\n",
      "Epoch: 3000 Lost= 0.008435308\n",
      "Epoch: 3050 Lost= 0.006630486\n",
      "Epoch: 3100 Lost= 0.006219946\n",
      "Epoch: 3150 Lost= 0.010156503\n",
      "Epoch: 3200 Lost= 0.010969686\n",
      "Epoch: 3250 Lost= 0.008308355\n",
      "Epoch: 3300 Lost= 0.009059173\n",
      "Epoch: 3350 Lost= 0.007408781\n",
      "Epoch: 3400 Lost= 0.007822460\n",
      "Epoch: 3450 Lost= 0.008821481\n",
      "Epoch: 3500 Lost= 0.011563721\n",
      "Epoch: 3550 Lost= 0.008778165\n",
      "Epoch: 3600 Lost= 0.004861978\n",
      "Epoch: 3650 Lost= 0.009765644\n",
      "Epoch: 3700 Lost= 0.005604913\n",
      "Epoch: 3750 Lost= 0.008586363\n",
      "Epoch: 3800 Lost= 0.007880390\n",
      "Epoch: 3850 Lost= 0.006514195\n",
      "Epoch: 3900 Lost= 0.009532564\n",
      "Epoch: 3950 Lost= 0.011905278\n",
      "Epoch: 4000 Lost= 0.004496051\n",
      "Epoch: 4050 Lost= 0.009520737\n",
      "Epoch: 4100 Lost= 0.009341618\n",
      "Epoch: 4150 Lost= 0.011082591\n",
      "Epoch: 4200 Lost= 0.009795765\n",
      "Epoch: 4250 Lost= 0.009205178\n",
      "Epoch: 4300 Lost= 0.011326682\n",
      "Epoch: 4350 Lost= 0.006966408\n",
      "Epoch: 4400 Lost= 0.010267761\n",
      "Epoch: 4450 Lost= 0.009994171\n",
      "Epoch: 4500 Lost= 0.004844417\n",
      "Epoch: 4550 Lost= 0.008707413\n",
      "Epoch: 4600 Lost= 0.009254576\n",
      "Epoch: 4650 Lost= 0.007936132\n",
      "Epoch: 4700 Lost= 0.009423871\n",
      "Epoch: 4750 Lost= 0.006485488\n",
      "Epoch: 4800 Lost= 0.008348317\n",
      "Epoch: 4850 Lost= 0.005965868\n",
      "Epoch: 4900 Lost= 0.007654273\n",
      "Epoch: 4950 Lost= 0.007108074\n",
      "Epoch: 5000 Lost= 0.007423395\n",
      "Epoch: 5050 Lost= 0.005291300\n",
      "Epoch: 5100 Lost= 0.007367649\n",
      "Epoch: 5150 Lost= 0.007767571\n",
      "Epoch: 5200 Lost= 0.007553570\n",
      "Epoch: 5250 Lost= 0.009136152\n",
      "Epoch: 5300 Lost= 0.007933140\n",
      "Epoch: 5350 Lost= 0.008349122\n",
      "Epoch: 5400 Lost= 0.008287487\n",
      "Epoch: 5450 Lost= 0.008026359\n",
      "Epoch: 5500 Lost= 0.006435775\n",
      "Epoch: 5550 Lost= 0.010150034\n",
      "Epoch: 5600 Lost= 0.011546349\n",
      "Epoch: 5650 Lost= 0.007829445\n",
      "Epoch: 5700 Lost= 0.008780076\n",
      "Epoch: 5750 Lost= 0.006007530\n",
      "Epoch: 5800 Lost= 0.011941179\n",
      "Epoch: 5850 Lost= 0.011674953\n",
      "Epoch: 5900 Lost= 0.011097293\n",
      "Epoch: 5950 Lost= 0.006494733\n",
      "Epoch: 6000 Lost= 0.007486508\n",
      "Epoch: 6050 Lost= 0.007181453\n",
      "Epoch: 6100 Lost= 0.006772755\n",
      "Epoch: 6150 Lost= 0.008941226\n",
      "Epoch: 6200 Lost= 0.007849224\n",
      "Epoch: 6250 Lost= 0.004420795\n",
      "Epoch: 6300 Lost= 0.008801700\n",
      "Epoch: 6350 Lost= 0.006529342\n",
      "Epoch: 6400 Lost= 0.007945259\n",
      "Epoch: 6450 Lost= 0.006662032\n",
      "Epoch: 6500 Lost= 0.007999206\n",
      "Epoch: 6550 Lost= 0.007060499\n",
      "Epoch: 6600 Lost= 0.011879167\n",
      "Epoch: 6650 Lost= 0.009920239\n",
      "Epoch: 6700 Lost= 0.010142475\n",
      "Epoch: 6750 Lost= 0.006970206\n",
      "Epoch: 6800 Lost= 0.007773768\n",
      "Epoch: 6850 Lost= 0.006478016\n",
      "Epoch: 6900 Lost= 0.004464757\n",
      "Epoch: 6950 Lost= 0.009828147\n",
      "Epoch: 7000 Lost= 0.004658412\n",
      "Epoch: 7050 Lost= 0.006301315\n",
      "Epoch: 7100 Lost= 0.009272961\n",
      "Epoch: 7150 Lost= 0.005387214\n",
      "Epoch: 7200 Lost= 0.005233101\n",
      "Epoch: 7250 Lost= 0.008513723\n",
      "Epoch: 7300 Lost= 0.008468051\n",
      "Epoch: 7350 Lost= 0.007948940\n",
      "Epoch: 7400 Lost= 0.004539040\n",
      "Epoch: 7450 Lost= 0.008052011\n",
      "Epoch: 7500 Lost= 0.009329290\n",
      "Epoch: 7550 Lost= 0.012217518\n",
      "Epoch: 7600 Lost= 0.004673881\n",
      "Epoch: 7650 Lost= 0.006044763\n",
      "Epoch: 7700 Lost= 0.006840834\n",
      "Epoch: 7750 Lost= 0.010098041\n",
      "Epoch: 7800 Lost= 0.007633005\n",
      "Epoch: 7850 Lost= 0.005432201\n",
      "Epoch: 7900 Lost= 0.007209484\n",
      "Epoch: 7950 Lost= 0.008379992\n",
      "Epoch: 8000 Lost= 0.011357051\n",
      "Epoch: 8050 Lost= 0.007506899\n",
      "Epoch: 8100 Lost= 0.008670218\n",
      "Epoch: 8150 Lost= 0.009894054\n",
      "Epoch: 8200 Lost= 0.004777205\n",
      "Epoch: 8250 Lost= 0.008091902\n",
      "Epoch: 8300 Lost= 0.008712186\n",
      "Epoch: 8350 Lost= 0.010560947\n",
      "Epoch: 8400 Lost= 0.009256471\n",
      "Epoch: 8450 Lost= 0.009842318\n",
      "Epoch: 8500 Lost= 0.005614923\n",
      "Epoch: 8550 Lost= 0.009779747\n",
      "Epoch: 8600 Lost= 0.006716320\n",
      "Epoch: 8650 Lost= 0.004551542\n",
      "Epoch: 8700 Lost= 0.005712950\n",
      "Epoch: 8750 Lost= 0.005763089\n",
      "Epoch: 8800 Lost= 0.005090279\n",
      "Epoch: 8850 Lost= 0.007310494\n",
      "Epoch: 8900 Lost= 0.008737857\n",
      "Epoch: 8950 Lost= 0.009048175\n",
      "Epoch: 9000 Lost= 0.005778181\n",
      "Epoch: 9050 Lost= 0.011073316\n",
      "Epoch: 9100 Lost= 0.007801946\n",
      "Epoch: 9150 Lost= 0.008096802\n",
      "Epoch: 9200 Lost= 0.005892430\n",
      "Epoch: 9250 Lost= 0.007568222\n",
      "Epoch: 9300 Lost= 0.008175227\n",
      "Epoch: 9350 Lost= 0.007391698\n",
      "Epoch: 9400 Lost= 0.010110760\n",
      "Epoch: 9450 Lost= 0.006051093\n",
      "Epoch: 9500 Lost= 0.006957242\n",
      "Epoch: 9550 Lost= 0.007000905\n",
      "Epoch: 9600 Lost= 0.006110056\n",
      "Epoch: 9650 Lost= 0.006777785\n",
      "Epoch: 9700 Lost= 0.006502746\n",
      "Epoch: 9750 Lost= 0.007850058\n",
      "Epoch: 9800 Lost= 0.006959675\n",
      "Epoch: 9850 Lost= 0.007073931\n",
      "Epoch: 9900 Lost= 0.009634159\n",
      "Epoch: 9950 Lost= 0.008523479\n",
      "Epoch: 10000 Lost= 0.007527002\n",
      "Model saved in file: /tmp/model.ckpt\n",
      "Second Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# Running a new session\n",
    "print(\"Starting 2nd session...\")\n",
    "with tf.Session() as sess:\n",
    "    # Initialize variables\n",
    "    sess.run(init)\n",
    "\n",
    "    # Restore model weights from previously saved model\n",
    "    saver.restore(sess, model_path)\n",
    "    print(\"Model restored from file: %s\" % model_path)\n",
    "\n",
    "    # Resume training\n",
    "    for epoch in range(epochs):\n",
    "        rand_index = np.random.choice(len(X_train), size=batch_size)\n",
    "        X_rand = X_train_std[rand_index]\n",
    "        y_rand = np.transpose([y_train[rand_index]])\n",
    "        sess.run(optimizer, feed_dict={X: X_rand, y: y_rand})\n",
    "\n",
    "        train_temp_loss = sess.run(loss, feed_dict={X: X_rand, y: y_rand})\n",
    "        train_loss.append(np.sqrt(train_temp_loss))\n",
    "    \n",
    "        test_temp_loss = sess.run(loss, feed_dict={X: X_test_std, y: np.transpose([y_test])})\n",
    "        test_loss.append(np.sqrt(test_temp_loss))\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"Lost=\", \\\n",
    "                \"{:.9f}\".format(train_temp_loss))\n",
    "\n",
    "    # Close writer\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "    \n",
    "    # Save model weights to disk\n",
    "    save_path = saver.save(sess, model_path)\n",
    "    print(\"Model saved in file: %s\" % save_path)\n",
    "    print(\"Second Optimization Finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.2, 0.1,\n",
       "       0.1, 0.2, 0.4, 0.4, 0.3, 0.3, 0.3, 0.2, 0.4, 0.2, 0.5, 0.2, 0.2,\n",
       "       0.4, 0.2, 0.2, 0.2, 0.2, 0.4, 0.1, 0.2, 0.1, 0.2, 0.2, 0.1, 0.2,\n",
       "       0.2, 0.3, 0.3, 0.2, 0.6, 0.4, 0.3, 0.2, 0.2, 0.2, 0.2, 1.4, 1.5,\n",
       "       1.5, 1.3, 1.5, 1.3, 1.6, 1. , 1.3, 1.4, 1. , 1.5, 1. , 1.4, 1.3,\n",
       "       1.4, 1.5, 1. , 1.5, 1.1, 1.8, 1.3, 1.5, 1.2, 1.3, 1.4, 1.4, 1.7,\n",
       "       1.5, 1. , 1.1, 1. , 1.2, 1.6, 1.5, 1.6, 1.5, 1.3, 1.3, 1.3, 1.2,\n",
       "       1.4, 1.2, 1. , 1.3, 1.2, 1.3, 1.3, 1.1, 1.3, 2.5, 1.9, 2.1, 1.8,\n",
       "       2.2, 2.1, 1.7, 1.8, 1.8, 2.5, 2. , 1.9, 2.1, 2. , 2.4, 2.3, 1.8,\n",
       "       2.2, 2.3, 1.5, 2.3, 2. , 2. , 1.8, 2.1, 1.8, 1.8, 1.8, 2.1, 1.6,\n",
       "       1.9, 2. , 2.2, 1.5, 1.4, 2.3, 2.4, 1.8, 1.8, 2.1, 2.4, 2.3, 1.9,\n",
       "       2.3, 2.5, 2.3, 1.9, 2. , 2.3, 1.8])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use the model to make some predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prediction session...\n",
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n",
      "Model restored from file: /tmp/model.ckpt\n",
      "[[0.24097222]\n",
      " [0.19687119]\n",
      " [2.0788074 ]]\n"
     ]
    }
   ],
   "source": [
    "# Running a new session for predictions\n",
    "print(\"Starting prediction session...\")\n",
    "with tf.Session() as sess:\n",
    "    # Initialize variables\n",
    "    sess.run(init)\n",
    "\n",
    "    # Restore model weights from previously saved model\n",
    "    saver.restore(sess, model_path)\n",
    "    print(\"Model restored from file: %s\" % model_path)\n",
    "\n",
    "    # We try to predict the petal width (cm) of three samples\n",
    "    feed_dict = {X: [[5.1, 3.5, 1.4],\n",
    "                     [4.8, 3.0, 1.4],\n",
    "                     [6.3, 3.4, 5.6]]\n",
    "                }\n",
    "    prediction = sess.run(y_hat, feed_dict)\n",
    "    print(prediction) # True value 0.2, 0.1, 2.4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, better results, but still not very good results. We could try to improve them with a deeper network (more layers) or retouching the net parameters and number of neurons. That is another story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "deeptrading",
   "language": "python",
   "name": "deeptrading"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
