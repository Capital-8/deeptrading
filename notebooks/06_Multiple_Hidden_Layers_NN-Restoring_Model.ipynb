{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restoring a multiple hidden layer Neural Network \n",
    "\n",
    "\n",
    "![deep_network_model](../images/deep_network_model.png)\n",
    "\n",
    "\n",
    "The progress of the model can be saved during and after training. This means that a model can be resumed where it left off and avoid long training times. Saving also means that you can share your model and others can recreate your work.\n",
    "\n",
    "We will illustrate how to restore a multiple fully connected hidden layer NN. This NN have been saved in another place. We will restore the model from the \"model_path\" and make predictions with this so trained model after reload it.\n",
    "\n",
    "We will use the \"Concrete Compressive Strength Data Set\" from:\n",
    "https://archive.ics.uci.edu/ml/datasets/Concrete+Compressive+Strength\n",
    "\n",
    "We will build a three-hidden layer neural network  to predict the fourth attribute, Petal Width from the other three (Sepal length, Sepal width, Petal length)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parrondo/anaconda3/envs/deeptrading/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/parrondo/anaconda3/envs/deeptrading/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/parrondo/anaconda3/envs/deeptrading/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/parrondo/anaconda3/envs/deeptrading/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_iris\n",
    "from tensorflow.python.framework import ops\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Leave in blanck intentionally\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Leave in blanck intentionally\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Leave in blanck intentionally\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Leave in blanck intentionally\n",
    "#\n",
    "# If you have used data transformation becareful to ingest the transformed data to the model before\n",
    "# making predictions and back-transform the predicitions. See notebook 05.\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clears the default graph stack and resets the global default graph\n",
    "ops.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Placeholders\n",
      "Initializers\n"
     ]
    }
   ],
   "source": [
    "# make results reproducible\n",
    "seed = 2\n",
    "tf.set_random_seed(seed)\n",
    "np.random.seed(seed)  \n",
    "\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.005\n",
    "batch_size = 50\n",
    "n_features = 8#  Number of features in training data\n",
    "epochs = 10000\n",
    "display_step = 100\n",
    "model_path = \"../model/tmp/model.ckpt\"\n",
    "n_classes = 1\n",
    "\n",
    "# Network Parameters\n",
    "# See figure of the model\n",
    "d0 = D = n_features # Layer 0 (Input layer number of features)\n",
    "d1 = 5 # Layer 1 (5 hidden nodes)\n",
    "d2 = 15 # Layer 2 (15 hidden nodes) \n",
    "d3 = 5 # Layer 3 (5 hidden nodes)\n",
    "d4 = C = 1 # Layer 4 (Output layer)\n",
    "\n",
    "# tf Graph input\n",
    "print(\"Placeholders\")\n",
    "X = tf.placeholder(dtype=tf.float32, shape=[None, n_features], name=\"X\")\n",
    "y = tf.placeholder(dtype=tf.float32, shape=[None,n_classes], name=\"y\")\n",
    "\n",
    "\n",
    "# Initializers\n",
    "print(\"Initializers\")\n",
    "sigma = 1\n",
    "weight_initializer = tf.variance_scaling_initializer(mode=\"fan_avg\", distribution=\"uniform\", scale=sigma)\n",
    "bias_initializer = tf.zeros_initializer()\n",
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(X, variables):\n",
    "    # Hidden layer with ReLU activation\n",
    "    layer_1 = tf.nn.relu(tf.add(tf.matmul(X, variables['W1']), variables['bias1']))\n",
    "    # Hidden layer with ReLU activation\n",
    "    layer_2 = tf.nn.relu(tf.add(tf.matmul(layer_1, variables['W2']), variables['bias2']))\n",
    "    # Hidden layer with ReLU activation\n",
    "    layer_3 = tf.nn.relu(tf.add(tf.matmul(layer_2, variables['W3']), variables['bias3']))\n",
    "    # Output layer with ReLU activation\n",
    "    out_layer = tf.nn.relu(tf.add(tf.matmul(layer_3, variables['W4']), variables['bias4']))\n",
    "    return out_layer\n",
    "\n",
    "# Store layers weight & bias\n",
    "variables = {\n",
    "    'W1': tf.Variable(weight_initializer([n_features, d1]), name=\"W1\"), # inputs -> d1 hidden neurons\n",
    "    'bias1': tf.Variable(bias_initializer([d1]), name=\"bias1\"), # one biases for each d1 hidden neurons\n",
    "    'W2': tf.Variable(weight_initializer([d1, d2]), name=\"W2\"), # d1 hidden inputs -> d2 hidden neurons\n",
    "    'bias2': tf.Variable(bias_initializer([d2]), name=\"bias2\"), # one biases for each d2 hidden neurons\n",
    "    'W3': tf.Variable(weight_initializer([d2, d3]), name=\"W3\"), ## d2 hidden inputs -> d3 hidden neurons\n",
    "    'bias3': tf.Variable(bias_initializer([d3]), name=\"bias3\"), # one biases for each d3 hidden neurons\n",
    "    'W4': tf.Variable(weight_initializer([d3, d4]), name=\"W4\"), # d3 hidden inputs -> 1 output\n",
    "    'bias4': tf.Variable(bias_initializer([d4]), name=\"bias4\") # 1 bias for the output\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "y_hat = multilayer_perceptron(X, variables)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss = tf.reduce_mean(tf.square(y - y_hat)) # MSE\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss) # Train step\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 'Saver' op to save and restore all the variables\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model  and Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Leave in blanck intentionally\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving a Tensorflow model\n",
    "\n",
    "So, now we have our model saved.\n",
    "\n",
    "Tensorflow model has four main files:\n",
    "* a) Meta graph:\n",
    "This is a protocol buffer which saves the complete Tensorflow graph; i.e. all variables, operations, collections etc. This file has .meta extension.\n",
    "\n",
    "\n",
    "* b) y c) Checkpoint files:\n",
    "It is a binary file which contains all the values of the weights, biases, gradients and all the other variables saved. Tensorflow has changed from version 0.11. Instead of a single .ckpt file, we have now two files: .index and .data file that contains our training variables. \n",
    "\n",
    "\n",
    "* d) Along with this, Tensorflow also has a file named checkpoint which simply keeps a record of latest checkpoint files saved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use the model to make some predictions. First we transform our samples accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# You need notebook 05 to make this transform there.\n",
    "#sc.transform([[203.5, 305.3, 0.0, 203.5, 0.0, 963.4, 630.0, 90],\n",
    "#              [173.0, 116.0, 0.0, 192.0, 0.0, 946.8, 856.8, 90],\n",
    "#              [522.0, 0.0, 0.0, 146.0, 0.0, 896.0, 896.0, 7]]) #True value  51.86, 32.10, 50.51"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prediction session...\n",
      "INFO:tensorflow:Restoring parameters from ../model/tmp/model.ckpt\n",
      "Model restored from file: ../model/tmp/model.ckpt\n",
      "[[0.137997 ]\n",
      " [0.137997 ]\n",
      " [1.1572909]]\n"
     ]
    }
   ],
   "source": [
    "# Running a new session for predictions\n",
    "print(\"Starting prediction session...\")\n",
    "with tf.Session() as sess:\n",
    "    # Initialize variables\n",
    "    sess.run(init)\n",
    "\n",
    "    # Restore model weights from previously saved model\n",
    "    saver.restore(sess, model_path)\n",
    "    print(\"Model restored from file: %s\" % model_path)\n",
    "\n",
    "    # We try to predict the Concrete compressive strength (MPa megapascals) of three samples\n",
    "    feed_dict_std = {X: [[ 9.78564946, 15.74026644, -2.11773518,  9.78564946, -2.11773518,\n",
    "        54.23470101, 34.73303791,  3.14666097],\n",
    "       [ 8.00160409,  4.66748653, -2.11773518,  9.11297662, -2.11773518,\n",
    "        53.26371238, 47.99931622,  3.14666097],\n",
    "       [28.41576252, -2.11773518, -2.11773518,  6.42228525, -2.11773518,\n",
    "        50.29225322, 50.29225322, -1.70828215]]}\n",
    "    prediction = sess.run(y_hat, feed_dict_std)\n",
    "    print(prediction) #True value  51.86, 32.10, 50.51\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Revert transformation. Again you need notebook 05.\n",
    "#y_hat_rev = sc.inverse_transform(prediction)\n",
    "#y_hat_rev\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, we have restored and made some predictions. We have used a model trained in other place but saved in \"model_path\"."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "deeptrading",
   "language": "python",
   "name": "deeptrading"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
